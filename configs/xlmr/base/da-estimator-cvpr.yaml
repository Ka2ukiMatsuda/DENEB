seed: 12
monitor: kendall
metric_mode: max
early_stopping: True
patience: 1
min_delta: 0.0
save_top_k: 10
save_weights_only: False
min_epochs: 1
max_epochs: 100
gradient_clip_val: 1.0
gpus: 1
precision: 32
debug: cls
batch_size: 16
# accumulate_grad_batches: 4
accumulate_grad_batches: 1
loader_workers: 4
optimizer: Adam
learning_rate: 3.0e-06
# learning_rate: 1.0e-06
encoder_learning_rate: 1.0e-05
layerwise_decay: 0.95
nr_frozen_epochs: 100000
# scheduler: constant
scheduler: linear_warmup
warmup_steps: 1000

train_path: data_en/nebula/train.csv
val_path: data_en/nebula/validation.csv
test_path: data_en/nebula/test.csv
train_img_dir_path: data_en/nebula/images
val_img_dir_path: data_en/nebula/images
test_img_dir_path: data_en/nebula/images

model: DenebEstimator
loss: huber
encoder_model: BERT
# pretrained_model: princeton-nlp/sup-simcse-roberta-large
pretrained_model: princeton-nlp/sup-simcse-roberta-base
layer: mix
scalar_mix_dropout: 0.1
pool: cls
dropout: 0.1
activations: Tanh
hidden_sizes: "1536, 768"
final_activation: False
